{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for Stock Trading\n",
    "\n",
    "This notebook has been cleaned up and gotten comments added to it by AI. Code was also written with help from AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports that we need to read the data and run simple eda\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "path = \"/home/tjb73/palmer_scratch/otdhcnysozsxek2a.csv\"\n",
    "test_path = \"/home/tjb73/palmer_scratch/test\"\n",
    "train_path = \"/home/tjb73/palmer_scratch/train_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data and understand some of it\n",
    "TESTING = True\n",
    "SMALL_DS = True\n",
    "\n",
    "# load in the df with the data\n",
    "df = pd.read_csv(\n",
    "    path, \n",
    "    parse_dates=[\"date\"],\n",
    "    nrows=(200000 if TESTING else 2000000) if SMALL_DS else 10_000_000,\n",
    "    low_memory=False,  # Suppress mixed type warning\n",
    "    dtype={'SYM_SUFFIX': str}  # Explicitly handle mixed type column\n",
    ") \n",
    "\n",
    "# show info about the df\n",
    "display(df.head())\n",
    "print(\"\\nrows:\", len(df), \"| columns:\", df.shape[1])\n",
    "# df.info(show_counts=True)\n",
    "\n",
    "# the lookback is the number of days to look back for the target return\n",
    "LOOKBACK = 25 if not TESTING else 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and feature engineering script with debug prints\n",
    "import pandas as pd, numpy as np, gc, os\n",
    "from pandas.api.types import is_sparse, is_string_dtype\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"0. Starting feature engineering script\")\n",
    "\n",
    "# 1. Define constants\n",
    "print(\"1. Defining constants\")\n",
    "ID_COLS = [\"PERMNO\",\"PERMCO\",\"CUSIP\",\"NCUSIP\",\"ISSUNO\",\"TSYMBOL\",\"COMNAM\",\n",
    "           \"NAMEENDT\",\"SHRCLS\",\"DCLRDT\",\"DLPRC\",\"DLRET\",\"DLRETX\",\"DLAMT\",\n",
    "           \"DLSTCD\",\"DLPDT\",\"NEXTDT\",\"PAYDT\",\"RCRDDT\",\"NWPERM\",\"ACPERM\",\n",
    "           \"ACCOMP\"]\n",
    "CATEGORICALS = [\"SHRCD\",\"EXCHCD\",\"HEXCD\",\"SICCD\",\"HSICCD\",\"HSICMG\",\"HSICIG\",\n",
    "                \"NAICS\",\"PRIMEXCH\",\"TRDSTAT\",\"SECSTAT\"]\n",
    "PRICE_COLS = [\"PRC\",\"OPENPRC\",\"ASKHI\",\"BIDLO\",\"BID\",\"ASK\"]\n",
    "NUMERIC_KEEP = [\"VOL\",\"NUMTRD\",\"SHROUT\",\"CFACPR\",\"CFACSHR\",\n",
    "                \"VWRETD\",\"VWRETX\",\"EWRETD\",\"EWRETX\",\"SPRTRN\"]\n",
    "\n",
    "# 2. Prepare input dataframe\n",
    "print(\"2. Preparing input dataframe\")\n",
    "df.columns = df.columns.str.upper()\n",
    "print(\"  Columns uppercased\")\n",
    "\n",
    "# 2a. Compute target return\n",
    "print(\"  Computing TARGET_RET\")\n",
    "df[\"RET\"]  = pd.to_numeric(df[\"RET\"],  errors=\"coerce\")\n",
    "df[\"RETX\"] = pd.to_numeric(df[\"RETX\"], errors=\"coerce\")\n",
    "df[\"TARGET_RET\"] = df[\"RET\"].fillna(df[\"RETX\"]).fillna(0.0)\n",
    "df.drop(columns=[\"RET\",\"RETX\"], inplace=True)\n",
    "print(\"  TARGET_RET done, dropped RET/RETX\")\n",
    "\n",
    "# 2b. Clean PRICE_COLS\n",
    "print(\"  Cleaning price columns\")\n",
    "for c in PRICE_COLS:\n",
    "    if c in df:\n",
    "        print(f\"  Processing {c}\")\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        s[s <= 0] = np.nan\n",
    "        df[c] = s\n",
    "\n",
    "# 2c. Create price ratios\n",
    "print(\"  Creating price ratio columns\")\n",
    "for base in (\"OPENPRC\",\"ASKHI\",\"BIDLO\",\"ASK\",\"BID\"):\n",
    "    if base in df and \"PRC\" in df:\n",
    "        col = f\"{base}_RATIO\"\n",
    "        print(f\"  {col}\")\n",
    "        df[col] = df[base] / df[\"PRC\"]\n",
    "\n",
    "# 2d. Clean other numeric columns\n",
    "print(\"  Cleaning other numeric columns\")\n",
    "num_cols = [c for c in NUMERIC_KEEP if c in df]\n",
    "for c in num_cols:\n",
    "    print(f\"  {c}\")\n",
    "    s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    s[s <= 0] = np.nan\n",
    "    df[c] = s\n",
    "df[num_cols] = df[num_cols].fillna(0)\n",
    "print(\"  Filled NaNs with 0 for numeric keep columns\")\n",
    "\n",
    "ratio_cols = [c for c in df if c.endswith(\"_RATIO\")]\n",
    "df[ratio_cols] = df[ratio_cols].fillna(0)\n",
    "print(\"  Filled NaNs with 0 for ratio columns\")\n",
    "\n",
    "# 3. One-hot encode categoricals\n",
    "print(\"3. One-hot encoding categoricals\")\n",
    "enc = OneHotEncoder(sparse_output=True, handle_unknown=\"ignore\")\n",
    "cat_sparse = enc.fit_transform(df[CATEGORICALS].fillna(\"UNK\").astype(str))\n",
    "cat_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    cat_sparse, index=df.index,\n",
    "    columns=enc.get_feature_names_out(CATEGORICALS)\n",
    ")\n",
    "df.drop(columns=CATEGORICALS, inplace=True)\n",
    "gc.collect()\n",
    "print(\"  One-hot encoding complete, dropped original categoricals\")\n",
    "\n",
    "# 4. Scale numeric features\n",
    "print(\"4. Scaling numeric features\")\n",
    "scale_cols = num_cols + ratio_cols\n",
    "df[scale_cols] = RobustScaler().fit_transform(df[scale_cols]).astype(\"float32\")\n",
    "print(f\"  Scaled columns: {len(scale_cols)} features\")\n",
    "\n",
    "# 4b. Combine with keep_cols\n",
    "keep_cols = [\"DATE\",\"TICKER\",\"PRC\",\"TARGET_RET\"]\n",
    "df = pd.concat([df[keep_cols + scale_cols], cat_df], axis=1)\n",
    "print(f\"  Concatenated base + engineered features; df shape now {df.shape}\")\n",
    "\n",
    "# 5. Handle remaining NaNs\n",
    "print(\"5. Patching remaining NaNs\")\n",
    "dense_cols = [c for c in df.columns if df[c].dtype.kind in (\"f\", \"i\", \"u\")]\n",
    "dense_nonsparse = [c for c in dense_cols if not is_sparse(df[c].dtype)]\n",
    "dense_sparse    = [c for c in dense_cols if  is_sparse(df[c].dtype)]\n",
    "print(f\"  Dense non-sparse: {len(dense_nonsparse)}, sparse: {len(dense_sparse)}\")\n",
    "\n",
    "df[dense_nonsparse] = df[dense_nonsparse].fillna(df[dense_nonsparse].median())\n",
    "print(\"  Filled NaNs in dense non-sparse with median\")\n",
    "df[dense_sparse]    = df[dense_sparse].fillna(0)\n",
    "print(\"  Filled NaNs in dense sparse with 0\")\n",
    "\n",
    "for c in df.columns.difference(dense_cols):\n",
    "    if is_string_dtype(df[c]) or df[c].dtype == object:\n",
    "        df[c] = df[c].fillna(\"\")\n",
    "print(\"  Filled NaNs in string columns with empty string\")\n",
    "\n",
    "sparse_cols = [c for c in df.columns if is_sparse(df[c].dtype)]\n",
    "df[sparse_cols] = df[sparse_cols].fillna(0)\n",
    "print(\"  Ensured sparse columns have no NaNs\")\n",
    "\n",
    "# Define feature columns\n",
    "FEAT_COLS = [c for c in df.columns if c not in keep_cols and df[c].dtype.kind in (\"f\",\"i\",\"u\")]\n",
    "print(f\"  Feature columns count: {len(FEAT_COLS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Convert DATE column to datetime if not already\n",
    "# df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "\n",
    "# Get unique sorted dates from 1980 onwards\n",
    "dates = df[\"DATE\"]  # already datetime\n",
    "mask = dates >= pd.Timestamp(\"1980-01-01\")  # build filter\n",
    "all_dates = sorted(dates.loc[mask].dropna().unique())  # unique + sort -> list\n",
    "\n",
    "if TESTING:\n",
    "    # keep only last 10% for quicker debugging\n",
    "    split = int(len(all_dates) * 0.9)\n",
    "    all_dates = all_dates[split:]\n",
    "\n",
    "print(f\"> Total unique dates for sampling: {len(all_dates)}\")\n",
    "\n",
    "# Split dates 85/15 for train/test\n",
    "cut = int(len(all_dates) * 0.85)\n",
    "train_dates = all_dates[:cut]\n",
    "test_dates = all_dates[cut:]\n",
    "\n",
    "print(f\"> Train dates: {len(train_dates)}, Test dates: {len(test_dates)}\")\n",
    "\n",
    "keep_dates = set(all_dates)\n",
    "\n",
    "# Filter dataframe if in testing mode\n",
    "if TESTING:\n",
    "    keep_dates = set(all_dates)\n",
    "    df = df.loc[df[\"DATE\"].isin(keep_dates)].copy()\n",
    "    print(f\"> Filtered df -> {df.shape[0]} rows over {len(keep_dates)} dates\")\n",
    "\n",
    "# Drop all rows before 2000\n",
    "df = df[df[\"DATE\"] >= pd.Timestamp(\"2000-01-01\")]\n",
    "print(f\"> After dropping pre-2000: {df.shape[0]} rows, from {df['DATE'].min().date()} to {df['DATE'].max().date()}\")\n",
    "\n",
    "# Set multi-index on DATE and TICKER\n",
    "df.set_index([\"DATE\", \"TICKER\"], inplace=True)\n",
    "print(\"> MultiIndex on (DATE, TICKER) established\")\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Build by_date dictionary using index-based lookup\n",
    "by_date = {}\n",
    "for date in tqdm(sorted(keep_dates), desc=\"Building by_date\"):\n",
    "    sub = df.loc[date, FEAT_COLS + [\"TARGET_RET\"]]\n",
    "    by_date[date] = sub\n",
    "\n",
    "print(f\"> Built by_date lookup for {len(by_date)} days\")\n",
    "\n",
    "# Save by_date dictionary to disk\n",
    "out_file = Path(\"by_date.pkl\")\n",
    "with out_file.open(\"wb\") as f:\n",
    "    pickle.dump(by_date, f)\n",
    "print(f\"> Saved by_date dictionary to {out_file.resolve()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save samples to disk\n",
    "def make_samples_to_disk(dates, out_dir, prefix):\n",
    "    print(f\"make_samples_to_disk: {prefix} — start\")\n",
    "    dates = pd.DatetimeIndex(dates).intersection(by_date.keys())\n",
    "    print(f\"    {len(dates)} dates intersected with by_date\")\n",
    "    if len(dates) < LOOKBACK + 1:\n",
    "        print(f\"Warning: {prefix}: not enough dates ({len(dates)})\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    pending, max_tkr, sid = [], 0, 0\n",
    "    feat_cols = FEAT_COLS  # cache locally\n",
    "\n",
    "    for i in tqdm(range(LOOKBACK, len(dates) - 1), desc=f\"build {prefix}\"):\n",
    "        win, nxt = dates[i - LOOKBACK:i], dates[i]\n",
    "        if any(d not in by_date for d in (*win, nxt)):\n",
    "            continue\n",
    "\n",
    "        # Align rows (tickers) first\n",
    "        mats  = [by_date[d][feat_cols] for d in win]\n",
    "        rets  = by_date[nxt][\"TARGET_RET\"]\n",
    "        common = set.intersection(*(set(m.index) for m in mats), set(rets.index))\n",
    "        if not common:\n",
    "            continue\n",
    "        tickers = sorted(common)\n",
    "\n",
    "        # Align columns and fill NA values\n",
    "        mats = [\n",
    "            m.loc[tickers].reindex(columns=feat_cols).astype(np.float32)\n",
    "              .to_numpy(copy=False)\n",
    "            for m in mats\n",
    "        ]   # every mat now has shape (len(tickers), len(FEAT_COLS))\n",
    "\n",
    "        cube = np.stack(mats)  # (L, N, F) — safe to stack\n",
    "        y    = rets.loc[tickers].to_numpy(np.float32)\n",
    "\n",
    "        # Mask invalid rows\n",
    "        good = (~np.isnan(cube).any((0, 2))) & (~np.isnan(y))\n",
    "        if not good.any():\n",
    "            continue\n",
    "        cube, y = cube[:, good, :], y[good]\n",
    "\n",
    "        # Scale older days\n",
    "        days = np.maximum([(win[-1] - d).days for d in win], 1)\n",
    "        cube *= (15.0 / np.array(days, np.float32))[:, None, None]\n",
    "\n",
    "        # Save un-padded file\n",
    "        fp = os.path.join(out_dir, f\"{prefix}_{sid:06d}.npz\")\n",
    "        np.savez(fp, X=cube, y=y)\n",
    "        pending.append((fp, cube.shape[1]))\n",
    "        max_tkr, sid = max(max_tkr, cube.shape[1]), sid + 1\n",
    "\n",
    "    if sid == 0:\n",
    "        print(f\"Warning: {prefix}: zero samples created\")\n",
    "        return\n",
    "\n",
    "    # Pad and compress files\n",
    "    for fp, t in tqdm(pending, desc=f\"pad {prefix}\"):\n",
    "        data = np.load(fp)\n",
    "        X, y = data[\"X\"], data[\"y\"]\n",
    "        L, F = X.shape[0], X.shape[2]\n",
    "\n",
    "        padX = np.full((L, max_tkr, F), np.nan, np.float32)\n",
    "        padX[:, :t] = X\n",
    "        padY = np.full(max_tkr, np.nan, np.float32)\n",
    "        padY[:t] = y\n",
    "        mask = np.zeros(max_tkr, bool)\n",
    "        mask[:t] = True\n",
    "\n",
    "        np.savez_compressed(fp, X=padX, y=padY, m=mask)\n",
    "\n",
    "    print(f\"Success: {prefix}: {sid} samples → {out_dir} (max tickers={max_tkr})\")\n",
    "\n",
    "\n",
    "# Write train/test samples to disk\n",
    "print(\"Writing train/test samples to disk\")\n",
    "make_samples_to_disk(train_dates, train_path, \"train\") \n",
    "make_samples_to_disk(test_dates,  test_path,  \"test\")\n",
    "print(\"Script complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_torch_gpu_memory():\n",
    "    # Delete any user-defined variables you no longer need\n",
    "    # Example: del model, optimizer, X_tr, y_tr, m_tr, etc.\n",
    "\n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Empty the CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Reset peak memory stats if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Example usage:\n",
    "clear_torch_gpu_memory()\n",
    "print(\"GPU memory after clear:\", torch.cuda.memory_allocated(), \"bytes allocated,\",\n",
    "      torch.cuda.memory_reserved(), \"bytes reserved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust REINFORCE implementation that streams samples from disk (.npz files)\n",
    "# No changes to hyperparameters or model except for the I/O pipeline\n",
    "import copy, time, math, glob, numpy as np, torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Hyperparameters and configuration\n",
    "USE_TX_COST   = False\n",
    "TX_COST       = 0.019\n",
    "EPOCHS        = 100\n",
    "PATIENCE      = 10\n",
    "BATCH         = 64\n",
    "LR            = 1e-4\n",
    "GAMMA         = 0.99\n",
    "ENTROPY_BETA  = 1e-3\n",
    "EMA_BETA      = 0.9\n",
    "CLIP_GRAD     = 1.0\n",
    "TOT_RET_RWD_SHARE = 0.9\n",
    "device        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "N_HEADS = 32\n",
    "N_LAYERS = 20\n",
    "if TESTING:\n",
    "    N_HEADS, N_LAYERS, EPOCHS = 8, 4, 5\n",
    "\n",
    "# Dataset class for loading .npz files from disk\n",
    "class NPZDataset(Dataset):\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.files = sorted(glob.glob(os.path.join(root_dir, \"*.npz\")))\n",
    "        assert self.files, f\"No .npz files found in {root_dir}\"\n",
    "        # Read one file to get dimensions\n",
    "        sample = np.load(self.files[0])\n",
    "        self.lookback, self.max_tkr, self.n_feat = sample[\"X\"].shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = np.load(self.files[idx])\n",
    "        X = torch.from_numpy(d[\"X\"])\n",
    "        y = torch.from_numpy(d[\"y\"])\n",
    "        m = torch.from_numpy(d[\"m\"])\n",
    "        return X, y, m\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate(batch):\n",
    "    X = torch.stack([b[0] for b in batch]).float()\n",
    "    y = torch.stack([b[1] for b in batch]).float()\n",
    "    m = torch.stack([b[2] for b in batch]).bool()\n",
    "    return X.to(device), y.to(device), m.to(device)\n",
    "\n",
    "# Initialize data loaders\n",
    "train_ds = NPZDataset(train_path)\n",
    "test_ds  = NPZDataset(test_path)\n",
    "feat_dim = train_ds.n_feat\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH,\n",
    "                          shuffle=False, drop_last=False,\n",
    "                          num_workers=0, collate_fn=collate)\n",
    "\n",
    "# Policy network definition\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, feat_dim:int, d=128, h=N_HEADS,\n",
    "                 layers=N_LAYERS, ff=256):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(feat_dim, d)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d, h, ff, 0.1, batch_first=True, activation=\"gelu\")\n",
    "        self.enc  = nn.TransformerEncoder(enc_layer, layers)\n",
    "        self.head = nn.Linear(d, 1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: (B,L,T,F)\n",
    "        B, L, T, F = x.shape\n",
    "        x = self.proj(torch.nan_to_num(x))              # (B,L,T,d)\n",
    "        x = x.permute(0,2,1,3).reshape(B*T, L, -1)      # (B*T,L,d)\n",
    "        h = self.enc(x)                                 # (B*T,L,d)\n",
    "        logits = self.head(h).squeeze(-1)               # (B*T,L)\n",
    "        logits = logits.view(B, T, L)[:, :, -1]         # (B,T)\n",
    "        return logits.masked_fill(~mask, -1e9)\n",
    "\n",
    "# Initialize model and optimizers\n",
    "policy = Policy(feat_dim).to(device)\n",
    "opt     = torch.optim.AdamW(policy.parameters(), lr=LR)\n",
    "sched   = CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)\n",
    "\n",
    "# Helper function to compute hard percentiles\n",
    "def hard_percentile(z, mask):\n",
    "    z = z.masked_fill(~mask, -1e9)\n",
    "    rank = z.argsort(dim=1).argsort(dim=1).float()\n",
    "    cnt  = mask.sum(dim=1, keepdim=True).clamp_min(1)\n",
    "    return torch.where(cnt>1, rank/(cnt-1), torch.zeros_like(rank))\n",
    "\n",
    "# Training loop\n",
    "baseline, best_R, best_state = 0.0, -float(\"inf\"), copy.deepcopy(policy.state_dict())\n",
    "print(f\"Device: {device} | Batches/epoch: {len(train_loader)}\")\n",
    "\n",
    "for ep in trange(1, EPOCHS+1, desc=\"Overall epochs\"):\n",
    "    policy.train()\n",
    "    ep_rewards, ep_netrets, t0 = [], [], time.time()\n",
    "\n",
    "    for Xb, yb, mb in tqdm(train_loader, desc=f\"Epoch {ep}\", leave=False):\n",
    "        # Forward pass\n",
    "        logits   = policy(Xb, mb)\n",
    "        pct_sig  = hard_percentile(logits, mb)\n",
    "        pct_ret  = hard_percentile(yb,    mb)\n",
    "\n",
    "        # Calculate rewards\n",
    "        align_r  = 1.0 - (pct_sig - pct_ret).abs().mean(dim=1)\n",
    "        best_idx = logits.argmax(dim=1, keepdim=True)\n",
    "        bonus_r  = yb.gather(1, best_idx).squeeze(1)\n",
    "\n",
    "        # Calculate transaction costs if enabled\n",
    "        cost = (TX_COST * pct_sig.abs().mean(dim=1)\n",
    "                if USE_TX_COST else torch.zeros_like(align_r))\n",
    "\n",
    "        # Combine rewards\n",
    "        reward = (TOT_RET_RWD_SHARE * align_r +\n",
    "                  (1-TOT_RET_RWD_SHARE) * bonus_r - cost)\n",
    "\n",
    "        # Calculate discounted returns\n",
    "        disc = GAMMA ** torch.arange(reward.size(0)-1, -1, -1, device=device)\n",
    "        G    = (reward.flip(0).cumsum(0).flip(0)) / disc\n",
    "        with torch.no_grad():\n",
    "            baseline = EMA_BETA*baseline + (1-EMA_BETA)*G.mean().item()\n",
    "        adv = G - baseline\n",
    "\n",
    "        # Calculate loss\n",
    "        logp    = F.log_softmax(logits, dim=1)\n",
    "        logprob = (logp * pct_sig.detach()).sum(dim=1)\n",
    "        entropy = -(logp.exp()*logp).sum(dim=1).mean()\n",
    "\n",
    "        loss = -(logprob * adv.detach()).mean() - ENTROPY_BETA * entropy\n",
    "        if torch.isnan(loss):\n",
    "            continue                               # Skip pathological batch\n",
    "\n",
    "        # Backward pass\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(policy.parameters(), CLIP_GRAD)\n",
    "        opt.step()\n",
    "\n",
    "        # Record metrics\n",
    "        ep_rewards.append(reward.mean().item())\n",
    "        ep_netrets.append((bonus_r - cost).mean().item())\n",
    "\n",
    "    # Update learning rate and print metrics\n",
    "    sched.step()\n",
    "    avg_R   = float(np.mean(ep_rewards)) if ep_rewards else best_R\n",
    "    avg_net = float(np.mean(ep_netrets)) if ep_netrets else 0.0\n",
    "    print(f\"Ep {ep:02d} | AvgReward {avg_R:+.6f}  AvgNetRet {avg_net:+.4%}  \"\n",
    "          f\"LR {sched.get_last_lr()[0]:.5f}  Time {time.time()-t0:.1f}s\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_R > best_R + 1e-5:\n",
    "        best_R, best_state, epochs_no_improve = avg_R, copy.deepcopy(policy.state_dict()), 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {ep}\")\n",
    "            break\n",
    "\n",
    "# Load best model and finish\n",
    "policy.load_state_dict(best_state)\n",
    "print(\"Training complete. Best AvgReward:\", best_R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model parameters locally\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ensure directory exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# path for saving\n",
    "save_path = os.path.join(\"models\", \"best_policy_state.pt\")\n",
    "\n",
    "# save the state dict of the trained policy (best version)\n",
    "torch.save(policy.state_dict(), save_path)\n",
    "\n",
    "print(f\"✔️ Best policy parameters saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WINDOW_LEN = 182  # six months of trading days\n",
    "tb_daily = (1 + 0.03)**(1/252) - 1  # 3% annual T-bill rate converted to daily\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper functions for portfolio weight calculations\n",
    "def pct_weights_np(vals: np.ndarray, mask: np.ndarray, frac=0.2) -> np.ndarray:\n",
    "    # Calculate percentage weights based on value ranks\n",
    "    vals_m = np.where(mask, vals, -1e9)\n",
    "    ranks = np.argsort(np.argsort(vals_m, axis=1), axis=1).astype(float)\n",
    "    cnt = mask.sum(axis=1, keepdims=True).clip(min=1)\n",
    "    pct = ranks / (cnt - 1)\n",
    "    long = pct >= (1 - frac)\n",
    "    short = pct <= frac\n",
    "    w = 0.5*long.astype(float) - 0.5*short.astype(float)\n",
    "    return np.where(mask, w, 0.0)\n",
    "\n",
    "def equal_weight(mask: np.ndarray) -> np.ndarray:\n",
    "    # Calculate equal weights for all assets\n",
    "    cnt = mask.sum(axis=1, keepdims=True).clip(min=1)\n",
    "    return np.where(mask, 1.0 / cnt, 0.0)\n",
    "\n",
    "def norm(arr: np.ndarray) -> np.ndarray:\n",
    "    # Normalize array by its first value\n",
    "    return arr / arr[0]\n",
    "\n",
    "def load_window(ds: NPZDataset, start: int, length: int):\n",
    "    # Load a contiguous window of samples from dataset\n",
    "    Xs, ys, ms = [], [], []\n",
    "    for j in range(start, start + length):\n",
    "        X, y, m = ds[j]\n",
    "        Xs.append(X)\n",
    "        ys.append(y)\n",
    "        ms.append(m)\n",
    "    return (torch.stack(Xs),           # (length, L, T, F)\n",
    "            torch.stack(ys),           # (length, T)\n",
    "            torch.stack(ms))           # (length, T)\n",
    "\n",
    "def plot_simulations_with_tb(n_sims: int = 5):\n",
    "    # Main function to run and plot portfolio simulations\n",
    "    for sim in range(1, n_sims + 1):\n",
    "        # Select random 6-month window\n",
    "        start_idx = random.randint(0, len(test_ds) - WINDOW_LEN - 1)\n",
    "        if WINDOW_LEN <= LOOKBACK + 2:\n",
    "            continue\n",
    "\n",
    "        # Load data for simulation window\n",
    "        Xw, yw, mw = load_window(test_ds, start_idx, WINDOW_LEN)\n",
    "        y_np = yw.cpu().numpy()\n",
    "        mask_np = mw.cpu().numpy().astype(bool)\n",
    "        _, N_tkr = y_np.shape\n",
    "\n",
    "        # Calculate portfolio weights for different strategies\n",
    "        with torch.no_grad():\n",
    "            logits = policy(Xw.to(device), mw.to(device)).cpu().numpy()\n",
    "        w_learn = pct_weights_np(logits, mask_np)\n",
    "\n",
    "        # Prior-day strategy weights\n",
    "        prev_ret = np.vstack([np.zeros((1, N_tkr)), y_np[:-1]])\n",
    "        w_prev = pct_weights_np(prev_ret, mask_np)\n",
    "\n",
    "        # EWMA strategy weights\n",
    "        ewm_ret = (pd.DataFrame(y_np)\n",
    "                  .ewm(alpha=0.98, adjust=False)\n",
    "                  .mean().shift(1).fillna(0).values)\n",
    "        w_ewma = pct_weights_np(ewm_ret, mask_np)\n",
    "\n",
    "        # Equal weight strategy\n",
    "        w_eq = equal_weight(mask_np)\n",
    "\n",
    "        # Portfolio simulation function\n",
    "        def simulate(w_mat):\n",
    "            ret_raw = np.nansum(w_mat * y_np, axis=1)\n",
    "            w_shift = np.vstack([np.zeros((1, N_tkr)), w_mat[:-1]])\n",
    "            turnover = np.abs(w_mat - w_shift).sum(axis=1)\n",
    "            ret_net = ret_raw - TX_COST * turnover\n",
    "            pv_raw = norm(np.cumprod(1 + ret_raw))\n",
    "            pv_net = norm(np.cumprod(1 + ret_net))\n",
    "            return pv_raw, pv_net, ret_raw, ret_net\n",
    "\n",
    "        # Run simulations for each strategy\n",
    "        pv_Lr, pv_Ln, ret_Lr, ret_Ln = simulate(w_learn)\n",
    "        pv_Pr, pv_Pn, ret_Pr, ret_Pn = simulate(w_prev)\n",
    "        pv_Er, pv_En, ret_Er, ret_En = simulate(w_ewma)\n",
    "        pv_EQr, pv_EQn, ret_EQr, ret_EQn = simulate(w_eq)\n",
    "\n",
    "        # Calculate T-bill benchmark\n",
    "        pv_tb = norm((1 + tb_daily) ** np.arange(1, len(pv_Lr) + 1))\n",
    "        ret_tb = np.full_like(ret_Lr, tb_daily)\n",
    "\n",
    "        # Calculate Sharpe ratios\n",
    "        def sharpe(ret_series):\n",
    "            excess = ret_series - tb_daily\n",
    "            return (excess.mean() / excess.std()) * np.sqrt(252)\n",
    "\n",
    "        sr = {\n",
    "            \"Learned\": sharpe(ret_Lr),\n",
    "            \"Prior-day\": sharpe(ret_Pr),\n",
    "            \"EWMA\": sharpe(ret_Er),\n",
    "            \"Equal-weight\": sharpe(ret_EQr),\n",
    "        }\n",
    "        sr_net = {\n",
    "            \"Learned\": sharpe(ret_Ln),\n",
    "            \"Prior-day\": sharpe(ret_Pn),\n",
    "            \"EWMA\": sharpe(ret_En),\n",
    "            \"Equal-weight\": sharpe(ret_EQn),\n",
    "        }\n",
    "\n",
    "        days = np.arange(len(pv_Lr))\n",
    "        start_lbl = f\"{start_idx}\"\n",
    "        end_lbl = f\"{start_idx + WINDOW_LEN - 1}\"\n",
    "\n",
    "        # Plot A: Performance without costs\n",
    "        fig, ax = plt.subplots(figsize=(8, 3), dpi=300)\n",
    "        ax.plot(days, pv_Lr, label=f\"Learned (SR={sr['Learned']:.2f})\", color=\"tab:blue\")\n",
    "        ax.plot(days, pv_Pr, label=f\"Prior-day (SR={sr['Prior-day']:.2f})\", color=\"tab:red\")\n",
    "        ax.plot(days, pv_Er, label=f\"EWMA (SR={sr['EWMA']:.2f})\", color=\"tab:orange\")\n",
    "        ax.plot(days, pv_EQr, label=f\"Equal-weight (SR={sr['Equal-weight']:.2f})\", color=\"purple\")\n",
    "        ax.plot(days, pv_tb, label=\"T-bill 3 %\", color=\"tab:green\", ls=\"--\")\n",
    "        ax.set_title(f\"Sim {sim} {start_lbl}→{end_lbl}  (No Costs)\")\n",
    "        ax.set_xlabel(\"Day\")\n",
    "        ax.set_ylabel(\"Normalized PV\")\n",
    "        ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "        fig.tight_layout(rect=[0,0,0.8,1])\n",
    "        fig.savefig(f\"./eda_plots/simulation_{sim}_no_cost_sr.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plot B: Performance with costs\n",
    "        fig, ax = plt.subplots(figsize=(8, 3), dpi=300)\n",
    "        ax.plot(days, pv_Ln, label=f\"Learned (SR={sr_net['Learned']:.2f})\", color=\"tab:blue\")\n",
    "        ax.plot(days, pv_Pn, label=f\"Prior-day (SR={sr_net['Prior-day']:.2f})\", color=\"tab:red\")\n",
    "        ax.plot(days, pv_En, label=f\"EWMA (SR={sr_net['EWMA']:.2f})\", color=\"tab:orange\")\n",
    "        ax.plot(days, pv_EQn, label=f\"Equal-weight (SR={sr_net['Equal-weight']:.2f})\", color=\"purple\")\n",
    "        ax.plot(days, pv_tb, label=\"T-bill 3 %\", color=\"tab:green\", ls=\"--\")\n",
    "        ax.set_title(f\"Sim {sim} {start_lbl}→{end_lbl}  (With Costs)\")\n",
    "        ax.set_xlabel(\"Day\")\n",
    "        ax.set_ylabel(\"Normalized PV\")\n",
    "        ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "        fig.tight_layout(rect=[0,0,0.8,1])\n",
    "        fig.savefig(f\"./eda_plots/simulation_{sim}_with_cost_sr.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plot C: Combined raw vs net performance\n",
    "        fig, ax = plt.subplots(figsize=(8, 3), dpi=300)\n",
    "        for label, col in [\n",
    "            (\"Learned\", \"tab:blue\"),\n",
    "            (\"Prior-day\", \"tab:red\"),\n",
    "            (\"EWMA\", \"tab:orange\"),\n",
    "            (\"Equal-weight\", \"purple\"),\n",
    "        ]:\n",
    "            ax.plot(days,\n",
    "                    {\"Learned\": pv_Lr, \"Prior-day\": pv_Pr, \"EWMA\": pv_Er, \"Equal-weight\": pv_EQr}[label],\n",
    "                    label=f\"{label} Raw (SR={sr[label]:.2f})\",\n",
    "                    color=col)\n",
    "            ax.plot(days,\n",
    "                    {\"Learned\": pv_Ln, \"Prior-day\": pv_Pn, \"EWMA\": pv_En, \"Equal-weight\": pv_EQn}[label],\n",
    "                    label=f\"{label} Net (SR={sr_net[label]:.2f})\",\n",
    "                    color=col, alpha=0.4)\n",
    "        ax.plot(days, pv_tb, label=\"T-bill 3 %\", color=\"tab:green\", ls=\"--\")\n",
    "        ax.set_title(f\"Sim {sim} {start_lbl}→{end_lbl}  (Raw vs Net All)\")\n",
    "        ax.set_xlabel(\"Day\"); ax.set_ylabel(\"Normalized PV\")\n",
    "        ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "        fig.tight_layout(rect=[0,0,0.8,1])\n",
    "        fig.savefig(f\"./eda_plots/simulation_{sim}_combined_all_sr.png\")\n",
    "        plt.show()\n",
    "\n",
    "# run the simulations with Sharpe-annotated plots\n",
    "plot_simulations_with_tb(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation code for portfolio strategies with and without trading costs\n",
    "# Includes functions for weight calculation and simulation, plus plotting utilities\n",
    "\n",
    "import os, random\n",
    "import numpy as np, torch, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# Create directory for plots\n",
    "os.makedirs(\"./eda_plots\", exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "WINDOW_LEN = 182  # six-month window length\n",
    "tb_daily = (1 + 0.03)**(1/252) - 1  # daily T-bill rate\n",
    "\n",
    "def pct_weights_np(vals, mask, frac=0.2):\n",
    "    # Calculate percentage weights based on value ranks\n",
    "    vals_m = np.where(mask, vals, -1e9)\n",
    "    ranks = np.argsort(np.argsort(vals_m, axis=1), axis=1).astype(float)\n",
    "    cnt = mask.sum(axis=1, keepdims=True).clip(min=1)\n",
    "    pct = ranks/(cnt - 1)\n",
    "    long = pct >= (1 - frac)\n",
    "    short = pct <= frac\n",
    "    return np.where(mask, 0.5*long - 0.5*short, 0.0)\n",
    "\n",
    "def simulate_once(start_idx):\n",
    "    # Run one simulation with different portfolio strategies\n",
    "    Xw, yw, mw = load_window(test_ds, start_idx, WINDOW_LEN)\n",
    "    y_np = yw.cpu().numpy()\n",
    "    m_np = mw.cpu().numpy().astype(bool)\n",
    "\n",
    "    # Learned strategy weights\n",
    "    with torch.no_grad():\n",
    "        logits = policy(Xw.to(device), mw.to(device)).cpu().numpy()\n",
    "    w_learn = pct_weights_np(logits, m_np)\n",
    "\n",
    "    # Prior-day strategy weights\n",
    "    prev_ret = np.vstack([np.zeros((1, y_np.shape[1])), y_np[:-1]])\n",
    "    w_prev = pct_weights_np(prev_ret, m_np)\n",
    "\n",
    "    # EWMA strategy weights\n",
    "    ewm = (pd.DataFrame(y_np)\n",
    "           .ewm(alpha=0.98, adjust=False).mean()\n",
    "           .shift(1).fillna(0).values)\n",
    "    w_ewma = pct_weights_np(ewm, m_np)\n",
    "\n",
    "    # Equal-weight strategy\n",
    "    cnt = m_np.sum(axis=1, keepdims=True).clip(min=1)\n",
    "    w_eq = np.where(m_np, 1.0 / cnt, 0.0)\n",
    "\n",
    "    def sim_pv(w):\n",
    "        # Calculate portfolio value with and without trading costs\n",
    "        ret_raw = np.nansum(w * y_np, axis=1)\n",
    "        w0 = np.vstack([np.zeros((1, w.shape[1])), w[:-1]])\n",
    "        trn = np.abs(w - w0).sum(axis=1)\n",
    "        ret_net = ret_raw - TX_COST * trn\n",
    "        return np.cumprod(1 + ret_raw), np.cumprod(1 + ret_net), ret_raw, ret_net\n",
    "\n",
    "    return sim_pv(w_learn), sim_pv(w_prev), sim_pv(w_ewma), sim_pv(w_eq)\n",
    "\n",
    "# Run 10 simulations\n",
    "n_sims = 10\n",
    "strats = [\"learn\", \"prior\", \"ewma\", \"eq\"]\n",
    "all_no, all_wc, ret_no_raw, ret_wc_raw, lengths = \\\n",
    "    {k: [] for k in strats}, {k: [] for k in strats}, \\\n",
    "    {k: [] for k in strats}, {k: [] for k in strats}, []\n",
    "\n",
    "for _ in range(n_sims):\n",
    "    start_idx = random.randint(0, len(test_ds) - WINDOW_LEN - 1)\n",
    "    (lr, ln, rr_Lr, rr_Ln), \\\n",
    "    (pr, pn, rr_Pr, rr_Pn), \\\n",
    "    (er, en, rr_Er, rr_En), \\\n",
    "    (qr, qn, rr_EQr, rr_EQn) = simulate_once(start_idx)\n",
    "\n",
    "    # Store results for each strategy\n",
    "    for k, arr in zip(strats, [lr, pr, er, qr]):\n",
    "        all_no[k].append(arr)\n",
    "    for k, arr in zip(strats, [ln, pn, en, qn]):\n",
    "        all_wc[k].append(arr)\n",
    "\n",
    "    for k, r in zip(strats, [rr_Lr, rr_Pr, rr_Er, rr_EQr]):\n",
    "        ret_no_raw[k].append(r)\n",
    "    for k, r in zip(strats, [rr_Ln, rr_Pn, rr_En, rr_EQn]):\n",
    "        ret_wc_raw[k].append(r)\n",
    "\n",
    "    lengths.append(len(lr))\n",
    "\n",
    "# Pad arrays to common length and normalize to starting value of 1\n",
    "max_len = max(lengths)\n",
    "def pad_and_norm(lst):\n",
    "    A = np.zeros((len(lst), max_len))\n",
    "    for i, a in enumerate(lst):\n",
    "        L = len(a)\n",
    "        A[i, :L] = a\n",
    "        A[i, L:] = a[-1]\n",
    "    return A / A[:, [0]]\n",
    "\n",
    "no_pad = {k: pad_and_norm(v) for k, v in all_no.items()}\n",
    "wc_pad = {k: pad_and_norm(v) for k, v in all_wc.items()}\n",
    "\n",
    "# Pad returns arrays (without normalization)\n",
    "def pad_ret(lst):\n",
    "    A = np.zeros((len(lst), max_len - 1))\n",
    "    for i, a in enumerate(lst):\n",
    "        L = len(a)\n",
    "        r = a[1:] / a[:-1] - 1  # Calculate returns\n",
    "        LL = len(r)\n",
    "        A[i, :LL] = r\n",
    "        A[i, LL:] = 0.0\n",
    "    return A\n",
    "\n",
    "ret_no = {k: pad_ret(ret_no_raw[k]) for k in strats}\n",
    "ret_wc = {k: pad_ret(ret_wc_raw[k]) for k in strats}\n",
    "\n",
    "# Calculate Sharpe ratios for each strategy\n",
    "sr_no = {}\n",
    "for k in strats:\n",
    "    ex = ret_no[k] - tb_daily\n",
    "    sr_sim = (ex.mean(axis=1) / ex.std(axis=1)) * np.sqrt(252)\n",
    "    sr_no[k] = float(sr_sim.mean())\n",
    "\n",
    "sr_wc = {}\n",
    "for k in strats:\n",
    "    ex = ret_wc[k] - tb_daily\n",
    "    sr_sim = (ex.mean(axis=1) / ex.std(axis=1)) * np.sqrt(252)\n",
    "    sr_wc[k] = float(sr_sim.mean())\n",
    "\n",
    "# Plotting parameters\n",
    "days = np.arange(1, max_len + 1)\n",
    "tb_norm = ((1 + tb_daily) ** days) / ((1 + tb_daily) ** days)[0]\n",
    "alpha_value = 0.01\n",
    "sample_size = 1000\n",
    "\n",
    "# Plot 1: No Trading Costs (without Sharpe ratios)\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "ax = plt.gca()\n",
    "for k, col in zip(strats, [\"tab:blue\", \"tab:red\", \"tab:orange\", \"purple\"]):\n",
    "    M = no_pad[k]\n",
    "    draw = min(sample_size, M.shape[0])\n",
    "    for arr in M[np.random.choice(M.shape[0], draw, replace=False)]:\n",
    "        ax.plot(days, arr, color=col, alpha=max(alpha_value, 1 / n_sims))\n",
    "    ax.plot(days, M.mean(0), color=col, lw=2, label=f\"{k.title()} mean\")\n",
    "ax.plot(days, tb_norm, color=\"tab:green\", ls=\"--\", label=\"T-bill 3%\")\n",
    "ax.set_title(\"Normalized PV — No Trading Costs\")\n",
    "ax.set_xlabel(\"Day\"); ax.set_ylabel(\"Normalized PV\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.01, 0.5), frameon=False)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.savefig(\"./eda_plots/10_simulations_normalized_no_cost.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: No Trading Costs (with Sharpe ratios)\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "ax = plt.gca()\n",
    "for k, col in zip(strats, [\"tab:blue\", \"tab:red\", \"tab:orange\", \"purple\"]):\n",
    "    M = no_pad[k]\n",
    "    draw = min(sample_size, M.shape[0])\n",
    "    for arr in M[np.random.choice(M.shape[0], draw, replace=False)]:\n",
    "        ax.plot(days, arr, color=col, alpha=max(alpha_value, 1 / n_sims))\n",
    "    ax.plot(days, M.mean(0), color=col, lw=2,\n",
    "            label=f\"{k.title()} mean (SR={sr_no[k]:.2f})\")\n",
    "ax.plot(days, tb_norm, color=\"tab:green\", ls=\"--\", label=\"T-bill 3%\")\n",
    "ax.set_title(\"Normalized PV — No Trading Costs (w/Sharpe)\")\n",
    "ax.set_xlabel(\"Day\"); ax.set_ylabel(\"Normalized PV\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.01, 0.5), frameon=False)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.savefig(\"./eda_plots/10_simulations_normalized_no_cost_sr.png\")\n",
    "plt.show()\n",
    "\n",
    "# ─────────────── PLOT: With Trading Costs (no SR) ──────────────────────────\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "ax = plt.gca()\n",
    "for k, col in zip(strats, [\"tab:blue\", \"tab:red\", \"tab:orange\", \"purple\"]):\n",
    "    M = wc_pad[k]\n",
    "    draw = min(sample_size, M.shape[0])\n",
    "    for arr in M[np.random.choice(M.shape[0], draw, replace=False)]:\n",
    "        ax.plot(days, arr, color=col, alpha=max(alpha_value, 1 / n_sims))\n",
    "    ax.plot(days, M.mean(0), color=col, lw=2, label=f\"{k.title()} mean\")\n",
    "ax.plot(days, tb_norm, color=\"tab:green\", ls=\"--\", label=\"T-bill 3%\")\n",
    "ax.set_title(\"Normalized PV — With Trading Costs\")\n",
    "ax.set_xlabel(\"Day\"); ax.set_ylabel(\"Normalized PV\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.01, 0.5), frameon=False)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.savefig(\"./eda_plots/10_simulations_normalized_with_cost.png\")\n",
    "plt.show()\n",
    "\n",
    "# ─────────────── PLOT: With Trading Costs (with SR) ────────────────────────\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "ax = plt.gca()\n",
    "for k, col in zip(strats, [\"tab:blue\", \"tab:red\", \"tab:orange\", \"purple\"]):\n",
    "    M = wc_pad[k]\n",
    "    draw = min(sample_size, M.shape[0])\n",
    "    for arr in M[np.random.choice(M.shape[0], draw, replace=False)]:\n",
    "        ax.plot(days, arr, color=col, alpha=max(alpha_value, 1 / n_sims))\n",
    "    ax.plot(days, M.mean(0), color=col, lw=2,\n",
    "            label=f\"{k.title()} mean (SR={sr_wc[k]:.2f})\")\n",
    "ax.plot(days, tb_norm, color=\"tab:green\", ls=\"--\", label=\"T-bill 3%\")\n",
    "ax.set_title(\"Normalized PV — With Trading Costs (w/Sharpe)\")\n",
    "ax.set_xlabel(\"Day\"); ax.set_ylabel(\"Normalized PV\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.01, 0.5), frameon=False)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.savefig(\"./eda_plots/10_simulations_normalized_with_cost_sr.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
